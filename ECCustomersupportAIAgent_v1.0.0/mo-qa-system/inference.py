# -*- coding: utf-8 -*-
"""
MonotaRO Q&A System - Model-based Inference Module
è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ãŸæº€è¶³åº¦äºˆæ¸¬
"""

import os
import sys
import random
import json

# Add parent directory to path
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
if parent_dir not in sys.path:
    sys.path.insert(0, parent_dir)
if os.path.join(parent_dir, "reproduce") not in sys.path:
    sys.path.insert(0, os.path.join(parent_dir, "reproduce"))
if os.path.join(parent_dir, "KG_tail_prediction") not in sys.path:
    sys.path.insert(0, os.path.join(parent_dir, "KG_tail_prediction"))

try:
    from TuckER_model import TuckER
    KG_AVAILABLE = True
except ImportError:
    KG_AVAILABLE = False
    print("[Warning] TuckER model not available")


# Load real product data from training
PRODUCT_DATA_PATH = os.path.join(current_dir, "product_data.json")
REAL_PRODUCT_DATA = {}
try:
    with open(PRODUCT_DATA_PATH, 'r', encoding='utf-8') as f:
        REAL_PRODUCT_DATA = json.load(f)
    print(f"[Inference] Loaded real product data: {len(REAL_PRODUCT_DATA)} categories")
except Exception as e:
    print(f"[Inference] Could not load product data: {e}")

# Optional imports
try:
    import torch
    import torch.nn as nn
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    print("[Warning] PyTorch not available")

try:
    from transformers import AutoTokenizer, AutoModel
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    print("[Warning] Transformers not available")

# ==========================================
# ã‚«ãƒ†ã‚´ãƒªå®šç¾© (24ã‚«ãƒ†ã‚´ãƒª)
# ==========================================
# ==========================================
# ã‚«ãƒ†ã‚´ãƒªå®šç¾© (JSONã‹ã‚‰å‹•çš„ãƒ­ãƒ¼ãƒ‰)
# ==========================================
CATEGORY_LIST = sorted(list(REAL_PRODUCT_DATA.keys()))

# å„ã‚«ãƒ†ã‚´ãƒªã®ä»£è¡¨å•†å“ (JSONã‹ã‚‰å‹•çš„ãƒ­ãƒ¼ãƒ‰)
CATEGORY_PRODUCTS = {}
for i, cat in enumerate(CATEGORY_LIST):
    CATEGORY_PRODUCTS[i] = sorted(list(REAL_PRODUCT_DATA[cat].keys()))



# ã‚«ãƒ†ã‚´ãƒªã®ä¾¡æ ¼å¸¯
CATEGORY_PRICES = {
    0: (1980, 4980), 1: (498, 2980), 2: (1280, 5980), 3: (198, 980),
    4: (9800, 39800), 5: (1480, 8980), 6: (980, 7980), 7: (4980, 34800),
    8: (498, 2480), 9: (580, 4980), 10: (298, 1980), 11: (1980, 14800),
    12: (498, 3980), 13: (298, 2480), 14: (98, 798), 15: (980, 7980),
    16: (1480, 9800), 17: (980, 5980), 18: (498, 3480), 19: (2980, 19800),
    20: (1480, 9800), 21: (298, 1980), 22: (498, 3980), 23: (498, 3980),
}

# æº€è¶³åº¦ãƒ©ãƒ™ãƒ«
SATISFACTION_LABELS = {
    0: {"label": "ä¸æº€", "emoji": "ğŸ˜", "class": "negative"},
    1: {"label": "æ™®é€š", "emoji": "ğŸ˜", "class": "neutral"},
    2: {"label": "æº€è¶³", "emoji": "ğŸ˜Š", "class": "positive"},
}

# ==========================================
# XLM-RoBERTa ãƒ¢ãƒ‡ãƒ«å®šç¾©
# ==========================================
if TORCH_AVAILABLE:
    class HighAccuracyClassifierV2(nn.Module):
        """XLM-RoBERTa ãƒ™ãƒ¼ã‚¹ã®æº€è¶³åº¦åˆ†é¡å™¨"""
        
        def __init__(self, backbone, hidden_size=768, topic_num=24, num_classes=3):
            super().__init__()
            self.backbone = backbone
            self.hidden_size = hidden_size
            self.topic_embed = nn.Embedding(topic_num, 128)
            
            self.classifier = nn.Sequential(
                nn.Linear(hidden_size + 128, 512),
                nn.LayerNorm(512),
                nn.GELU(),
                nn.Dropout(0.3),
                nn.Linear(512, 256),
                nn.LayerNorm(256),
                nn.GELU(),
                nn.Dropout(0.2),
                nn.Linear(256, num_classes)
            )
        
        def forward(self, input_ids, attention_mask, topics):
            outputs = self.backbone(input_ids, attention_mask=attention_mask)
            
            if hasattr(outputs, 'last_hidden_state'):
                cls_output = outputs.last_hidden_state[:, 0, :]
            else:
                cls_output = outputs[0][:, 0, :]
            
            topic_emb = self.topic_embed(topics)
            combined = torch.cat([cls_output, topic_emb], dim=-1)
            
            logits = self.classifier(combined)
            return logits


# ==========================================
# å¿œç­”ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼ˆå°Šæ•¬èªå¯¾å¿œï¼‰
# ==========================================
RESPONSE_TEMPLATES = {
    "price": [
        "ã“ã¡ã‚‰ã®{product}ã¯ç¨è¾¼ã¿{price}å††ã§ã”ã–ã„ã¾ã™ã€‚",
        "{product}ã®ãŠå€¤æ®µã¯{price}å††ï¼ˆç¨è¾¼ï¼‰ã¨ãªã£ã¦ãŠã‚Šã¾ã™ã€‚ã”æ¤œè¨ã„ãŸã ã‘ã¾ã™ã¨å¹¸ã„ã§ã™ã€‚",
        "ãŠå•ã„åˆã‚ã›ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚{price}å††ã§ã”æä¾›ã—ã¦ãŠã‚Šã¾ã™ã€‚",
    ],
    "stock": [
        "ã¯ã„ã€{product}ã¯åœ¨åº«ãŒã”ã–ã„ã¾ã™ã€‚ã™ãã«ãŠå±Šã‘å¯èƒ½ã§ã”ã–ã„ã¾ã™ã€‚",
        "ã“ã¡ã‚‰ã®å•†å“ã¯åœ¨åº«ååˆ†ã«ã”ã–ã„ã¾ã™ã€‚ã”å®‰å¿ƒãã ã•ã„ã€‚",
        "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚ç¾åœ¨åœ¨åº«ã‚’ç¢ºèªä¸­ã§ã”ã–ã„ã¾ã™ã€‚",
    ],
    "delivery": [
        "é€šå¸¸2ã€œ3å–¶æ¥­æ—¥ä»¥å†…ã«ãŠå±Šã‘ã„ãŸã—ã¾ã™ã€‚",
        "æœ€çŸ­ã§ç¿Œæ—¥ãŠå±Šã‘ãŒå¯èƒ½ã§ã”ã–ã„ã¾ã™ã€‚ï¼ˆåœ°åŸŸã«ã‚ˆã‚Šã¾ã™ï¼‰",
        "ã”æ³¨æ–‡ç¢ºå®šå¾Œã€3æ—¥ä»¥å†…ã«ç™ºé€ã•ã›ã¦ã„ãŸã ãã¾ã™ã€‚",
    ],
    "spec": [
        "{product}ã®è©³ç´°ä»•æ§˜ã«ã¤ã„ã¦ã”æ¡ˆå†…ã„ãŸã—ã¾ã™ã€‚ã”ä¸æ˜ç‚¹ãŒã”ã–ã„ã¾ã—ãŸã‚‰ãŠç”³ã—ä»˜ã‘ãã ã•ã„ã€‚",
        "ã‚µã‚¤ã‚ºã‚„ä»•æ§˜ã«ã¤ã„ã¦ã¯å•†å“ãƒšãƒ¼ã‚¸ã«ã¦ç¢ºèªã„ãŸã ã‘ã¾ã™ã€‚",
        "å„ç¨®ã‚µã‚¤ã‚ºã‚’å–ã‚Šæƒãˆã¦ãŠã‚Šã¾ã™ã€‚ã”å¸Œæœ›ã‚’ãŠçŸ¥ã‚‰ã›ãã ã•ã„ã€‚",
    ],
    "recommend": [
        "{category}ã‚«ãƒ†ã‚´ãƒªã§ã¯ã€{product}ãŒå¤§å¤‰äººæ°—ã§ã”ã–ã„ã¾ã™ã€‚",
        "ãŠå®¢æ§˜ã®ã”ç”¨é€”ã«åˆã‚ã›ã¦æœ€é©ãªå•†å“ã‚’ã”ææ¡ˆã„ãŸã—ã¾ã™ã€‚",
        "åˆã‚ã¦ã®ãŠå®¢æ§˜ã«ã¯{product}ãŒãŠã™ã™ã‚ã§ã”ã–ã„ã¾ã™ã€‚",
    ],
    "return": [
        "è¿”å“ãƒ»äº¤æ›ã«ã¤ãã¾ã—ã¦ã¯ã€å•†å“åˆ°ç€å¾Œ7æ—¥ä»¥å†…ã«ã”é€£çµ¡ã‚’ãŠé¡˜ã„ã„ãŸã—ã¾ã™ã€‚",
        "æœªé–‹å°ãƒ»æœªä½¿ç”¨ã®å ´åˆã€è¿”å“ã‚’æ‰¿ã£ã¦ãŠã‚Šã¾ã™ã€‚",
        "ä¸‡ãŒä¸€ã®ä¸è‰¯å“ã«ã¤ãã¾ã—ã¦ã¯ã€ç„¡å„Ÿã§äº¤æ›ã•ã›ã¦ã„ãŸã ãã¾ã™ã€‚",
    ],
    "quality": [
        "ã“ã¡ã‚‰ã®å•†å“ã¯å³æ ¼ãªå“è³ªç®¡ç†ã®ã‚‚ã¨è£½é€ ã•ã‚Œã¦ãŠã‚Šã¾ã™ã€‚",
        "JISè¦æ ¼ã«æº–æ‹ ã—ãŸé«˜å“è³ªãªå•†å“ã§ã”ã–ã„ã¾ã™ã€‚",
        "å¤šãã®ãŠå®¢æ§˜ã‹ã‚‰ã”å¥½è©•ã‚’ã„ãŸã ã„ã¦ãŠã‚Šã¾ã™ã€‚",
    ],
    "greeting": [
        "ã„ã‚‰ã£ã—ã‚ƒã„ã¾ã›ã€‚MonotaRO ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆã§ã”ã–ã„ã¾ã™ã€‚æœ¬æ—¥ã¯ã©ã®ã‚ˆã†ãªã”ç”¨ä»¶ã§ã—ã‚‡ã†ã‹ï¼Ÿ",
        "ãŠå•ã„åˆã‚ã›ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚ä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ã¯ã”ã–ã„ã¾ã™ã‹ï¼Ÿ",
        "ã”æ¥åº—ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚{category}ã‚«ãƒ†ã‚´ãƒªã®å•†å“ã‚’ã”æ¡ˆå†…ã„ãŸã—ã¾ã™ã€‚",
    ],
    "thanks": [
        "ã“ã¡ã‚‰ã“ãã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚ã¾ãŸã®ã”åˆ©ç”¨ã‚’å¿ƒã‚ˆã‚ŠãŠå¾…ã¡ã—ã¦ãŠã‚Šã¾ã™ã€‚",
        "ã”åˆ©ç”¨ã„ãŸã ãã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚ä»–ã«ã”è³ªå•ãŒã”ã–ã„ã¾ã—ãŸã‚‰ãŠæ°—è»½ã«ã©ã†ãã€‚",
        "ãŠå½¹ã«ç«‹ã¦ã¦å…‰æ „ã§ã”ã–ã„ã¾ã™ã€‚ä»Šå¾Œã¨ã‚‚MonotaROã‚’ã‚ˆã‚ã—ããŠé¡˜ã„ã„ãŸã—ã¾ã™ã€‚",
    ],
    "complaint": [
        "ã”ä¸ä¾¿ã‚’ãŠã‹ã‘ã—ã¦å¤§å¤‰ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚æ—©æ€¥ã«å¯¾å¿œã•ã›ã¦ã„ãŸã ãã¾ã™ã€‚",
        "ã”è¿·æƒ‘ã‚’ãŠã‹ã‘ã—ã€èª ã«ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚è©³ç´°ã‚’ãŠèã‹ã›ã„ãŸã ã‘ã¾ã™ã§ã—ã‚‡ã†ã‹ã€‚",
        "è²´é‡ãªã”æ„è¦‹ã‚’ã„ãŸã ãã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚æ”¹å–„ã«åŠªã‚ã¦ã¾ã„ã‚Šã¾ã™ã€‚",
    ],
    "fallback": [
        "ã‹ã—ã“ã¾ã‚Šã¾ã—ãŸã€‚ã”è³ªå•ã«ã¤ã„ã¦ç¢ºèªã•ã›ã¦ã„ãŸã ãã¾ã™ã€‚",
        "ã”è³ªå•ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚ã‚‚ã†å°‘ã—è©³ã—ããŠèã‹ã›ã„ãŸã ã‘ã¾ã™ã§ã—ã‚‡ã†ã‹ã€‚",
        "æ‰¿çŸ¥ã„ãŸã—ã¾ã—ãŸã€‚æ‹…å½“è€…ã‚ˆã‚ŠæŠ˜ã‚Šè¿”ã—ã”é€£çµ¡ã•ã›ã¦ã„ãŸã ãã¾ã™ã€‚",
    ],
}

# å°Šæ•¬èªãƒ»ä¸å¯§èªæ¤œå‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
POLITE_KEYWORDS = [
    "ã”ã–ã„ã¾ã™", "ã„ãŸã ", "ãã ã•", "ãŠã‚Šã¾ã™", "å­˜ã˜", "ç”³ã—",
    "ä¼ºã„", "ãŠé¡˜ã„", "æ‰¿çŸ¥", "ã‹ã—ã“ã¾ã‚Š", "æã‚Œå…¥ã‚Š",
]

# ãƒã‚¬ãƒ†ã‚£ãƒ–æ„Ÿæƒ…ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆã‚ˆã‚ŠåŒ…æ‹¬çš„ï¼‰
# ãƒã‚¬ãƒ†ã‚£ãƒ–æ„Ÿæƒ…ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆã‚ˆã‚ŠåŒ…æ‹¬çš„ï¼‰
# æ³¨æ„: å˜ãªã‚‹å•ã„åˆã‚ã›ï¼ˆè¿”å“æ–¹æ³•ãªã©ï¼‰ã¯ãƒã‚¬ãƒ†ã‚£ãƒ–ã«å«ã¾ãªã„ã‚ˆã†ã«é™¤å¤–
NEGATIVE_KEYWORDS = [
    # ç›´æ¥çš„ãªã‚¯ãƒ¬ãƒ¼ãƒ 
    "ã‚¯ãƒ¬ãƒ¼ãƒ ", "è‹¦æƒ…", "ãƒˆãƒ©ãƒ–ãƒ«",
    # å“è³ªã¸ã®ä¸æº€ï¼ˆ"å•é¡Œ"ã¯è³ªå•ã§ã‚‚ä½¿ã†ã®ã§é™¤å¤–ã€æ–‡è„ˆä¾å­˜ï¼‰
    "å£Šã‚Œ", "ä¸è‰¯", "æ•…éšœ", "ç ´æ", "æ±šã‚Œ", "å‚·", "å‹•ã‹ãªã„",
    # ã‚µãƒ¼ãƒ“ã‚¹ã¸ã®ä¸æº€
    "é…ã„", "å±Šã‹ãªã„", "å±Šã„ã¦ãªã„", "é–“é•", "é•ã†", "æ¥ãªã„",
    # æ„Ÿæƒ…è¡¨ç¾
    "æ€’", "è…¹ç«‹", "ã²ã©ã„", "æœ€æ‚ª", "æœ€ä½", "ãŒã£ã‹ã‚Š", "å¤±æœ›", "æ®‹å¿µ", "ãµã–ã‘",
    # å¼·ã„æ‹’çµ¶
    "äºŒåº¦ã¨", "é‡‘è¿”ã›", "è©æ¬º",
    # æ•¬èªã§ã®ä¸æº€è¡¨ç¾
    "ã„ãŸã ã‘ã¾ã›ã‚“", "å›°ã£ã¦", "ç´å¾—ã§ãã¾ã›ã‚“", "æ‰¿æœã—ã‹ã­ã¾ã™",
]

# ä¸å¯§èªãƒ»ã‚¯ãƒƒã‚·ãƒ§ãƒ³è¨€è‘‰ï¼ˆã“ã‚Œã‚‰ãŒå«ã¾ã‚Œã¦ã„ã¦ã‚‚ãƒã‚¬ãƒ†ã‚£ãƒ–ã¨åˆ¤å®šã—ãªã„ï¼‰
# ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡æ‘˜å¯¾å¿œ: ã€Œã™ã¿ã¾ã›ã‚“ã€è³ªå•ã§ã™ãŒã€ãªã©ã¯æ™®é€šï¼ˆNeutralï¼‰ã¨ã™ã¹ã
POLITE_IGNORE_KEYWORDS = [
    "ã™ã¿ã¾ã›ã‚“", "ã™ã„ã¾ã›ã‚“", "æã‚Œå…¥ã‚Šã¾ã™", "å¤±ç¤¼ã—ã¾ã™", "ã”ã‚ã‚“", 
    "ãŠå¿™ã—ã„ã¨ã“ã‚", "è³ªå•", "èããŸã„", "æ•™ãˆã¦",
]

# å®¢è¦³çš„ãªå•ã„åˆã‚ã›ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆã“ã‚Œã‚‰ã®ã¿ã®å ´åˆã¯ã€Œæ™®é€šã€ã¨åˆ¤å®šã™ã‚‹ï¼‰
OBJECTIVE_KEYWORDS = [
    # ä¾¡æ ¼ãƒ»è¦‹ç©
    "ä¾¡æ ¼", "å€¤æ®µ", "ã„ãã‚‰", "è¦‹ç©", "é‡‘é¡", "è²»ç”¨",
    # åœ¨åº«ãƒ»ç´æœŸ
    "åœ¨åº«", "ã‚ã‚‹ï¼Ÿ", "ãªã„ï¼Ÿ", "ç´æœŸ", "ã„ã¤", "æ—¥", "ç™ºé€", "é…é€", "å±Šã",
    # å•†å“ä»•æ§˜
    "ã‚µã‚¤ã‚º", "å¯¸æ³•", "é‡ã•", "é‡é‡", "ã‚¹ãƒšãƒƒã‚¯", "ä»•æ§˜", "æè³ª", "ç´ æ", "ä½¿ãˆã‚‹", "ç”¨é€”",
    "å–ã‚Šä»˜ã‘", "è¨­ç½®", "ä½¿ã„æ–¹", "æ–¹æ³•", "ã©ã†", "é›£ã—", "è€è·é‡",
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡æ‘˜å¯¾å¿œï¼šæŠ€è¡“ä»•æ§˜ãƒ»ãƒã‚¤ãƒ³ãƒˆ
    "å›è»¢æ•°", "rpm", "ãƒˆãƒ«ã‚¯", "é›»åœ§", "é›»æµ", "ãƒã‚¤ãƒ³ãƒˆ", "é‚„å…ƒ", "ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³", 
    "å•†å“", "ã«ã¤ã„ã¦",
    # æ‰‹ç¶šããƒ»æ›¸é¡
    "é ˜åæ›¸", "è«‹æ±‚æ›¸", "ã‚¤ãƒ³ãƒœã‚¤ã‚¹", "å®›å", "è¿”å“", "äº¤æ›", "ã‚­ãƒ£ãƒ³ã‚»ãƒ«", "æ³¨æ–‡", "å¤‰æ›´", "å¤‰ãˆ",
]

# ãƒã‚¸ãƒ†ã‚£ãƒ–æ„Ÿæƒ…ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
POSITIVE_KEYWORDS = [
    # æ„Ÿè¬
    "ã‚ã‚ŠãŒã¨ã†", "æ„Ÿè¬", "åŠ©ã‹ã‚Š", "ãŠã‹ã’",
    # æº€è¶³
    "æº€è¶³", "å¬‰ã—ã„", "å–œ", "è‰¯ã„", "ã„ã„", "ç´ æ™´ã‚‰ã—ã„", "æœ€é«˜", "å®Œç’§",
    # æ¨è–¦
    "ãŠã™ã™ã‚", "æ°—ã«å…¥", "ãƒªãƒ”ãƒ¼ãƒˆ", "ã¾ãŸè²·",
    # å“è³ªè©•ä¾¡
    "ã—ã£ã‹ã‚Š", "ä¸ˆå¤«", "ãã‚Œã„", "ç¶ºéº—",
    # æ•¬èªã§ã®ãƒã‚¸ãƒ†ã‚£ãƒ–è¡¨ç¾
    "åŠ©ã‹ã‚Šã¾ã—ãŸ", "ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸ", "æ„Ÿè¬ç”³ã—ä¸Šã’",
]


class MonotaROInference:
    """MonotaRO Q&A æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹ï¼‰"""
    
    def __init__(self, model_path=None):
        """åˆæœŸåŒ–"""
        self.device = 'cpu'
        self.model = None
        self.tokenizer = None
        self.model_loaded = False
        self.current_category = None
        self.current_product = None
        self.current_price = None  # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®å®Ÿä¾¡æ ¼
        self.dialogue_history = []
        
        # KG Model
        self.kg_model = None
        self.kg_e2id = {}
        self.kg_r2id = {}
        self.kg_id2e = {}
        self.kg_loaded = False
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹

        if model_path is None:
            model_path = os.path.join(parent_dir, "best_model_v2.pt")
        
        print(f"[Inference] Initializing...")
        
        if TORCH_AVAILABLE and TRANSFORMERS_AVAILABLE:
            self._load_model(model_path)
        else:
            print("[Inference] Using rule-based inference (PyTorch/Transformers not available)")
            
        if KG_AVAILABLE and TORCH_AVAILABLE:
            self._load_kg_model()

    
    def _load_model(self, model_path):
        """ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ (run_full_pipeline.pyã¨åŒã˜æ§‹æˆ)"""
        try:
            # GPUåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
            if torch.cuda.is_available():
                self.device = 'cuda'
            elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                self.device = 'mps'
            print(f"[Inference] Using device: {self.device}")
            
            backbone = None
            hidden_size = 768
            
            # 1. Try XLM-RoBERTa (Primary)
            try:
                print("[Inference] Trying to load XLM-RoBERTa...")
                self.tokenizer = AutoTokenizer.from_pretrained("xlm-roberta-base")
                backbone = AutoModel.from_pretrained("xlm-roberta-base")
                hidden_size = backbone.config.hidden_size
                print("[Inference] Loaded XLM-RoBERTa")
            except Exception as e:
                print(f"[Inference] XLM-RoBERTa failed: {e}")
                
                # 2. Try BERT Japanese (Secondary)
                try:
                    print("[Inference] Trying to load cl-tohoku/bert-base-japanese-v2...")
                    self.tokenizer = AutoTokenizer.from_pretrained("cl-tohoku/bert-base-japanese-v2")
                    backbone = AutoModel.from_pretrained("cl-tohoku/bert-base-japanese-v2")
                    hidden_size = backbone.config.hidden_size
                    print("[Inference] Loaded cl-tohoku/bert-base-japanese-v2")
                except Exception as e2:
                    print(f"[Inference] BERT-Japanese failed: {e2}")
                    
                    # 3. Fallback to Multilingual BERT
                    print("[Inference] Falling back to Multilingual BERT...")
                    self.tokenizer = AutoTokenizer.from_pretrained("bert-base-multilingual-cased")
                    backbone = AutoModel.from_pretrained("bert-base-multilingual-cased")
                    hidden_size = 768

            # å­¦ç¿’æ™‚ã¨åŒã˜ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³
            self.tokenizer.add_special_tokens({'additional_special_tokens': ['[NO_TOKEN]']})
            backbone.resize_token_embeddings(len(self.tokenizer))
            
            # ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰
            self.model = HighAccuracyClassifierV2(
                backbone=backbone, 
                hidden_size=hidden_size, 
                topic_num=24, 
                num_classes=3
            ).to(self.device)
            
            # å­¦ç¿’æ¸ˆã¿é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰
            if os.path.exists(model_path):
                print(f"[Inference] Loading weights from {model_path}")
                state_dict = torch.load(model_path, map_location=self.device, weights_only=True)
                # strict=Falseã§äº’æ›æ€§ã®ã‚ã‚‹é‡ã¿ã®ã¿ãƒ­ãƒ¼ãƒ‰
                incompatible = self.model.load_state_dict(state_dict, strict=False)
                if incompatible.missing_keys:
                    print(f"[Inference] Missing keys: {len(incompatible.missing_keys)}")
                if incompatible.unexpected_keys:
                    print(f"[Inference] Unexpected keys: {len(incompatible.unexpected_keys)}")
                self.model_loaded = True
                print("[Inference] Model loaded successfully!")
            else:
                print(f"[Inference] Model file not found: {model_path}")
                print("[Inference] Using randomly initialized weights")
            
            self.model.eval()
            
        except Exception as e:
            print(f"[Inference] Error loading model: {e}")
            import traceback
            traceback.print_exc()
            self.model_loaded = False
            
    def _load_kg_model(self):
        """çŸ¥è­˜ã‚°ãƒ©ãƒ•ãƒ¢ãƒ‡ãƒ« (TuckER) ã‚’ãƒ­ãƒ¼ãƒ‰"""
        try:
            import pickle
            kg_dir = os.path.join(parent_dir, 'KG_tail_prediction')
            mapping_path = os.path.join(kg_dir, 'data/kg_mappings.pkl')
            model_path = os.path.join(kg_dir, 'model/TuckER_model_trained.pkl')
            
            if os.path.exists(mapping_path) and os.path.exists(model_path):
                print(f"[Inference] Loading KG mappings from {mapping_path}")
                with open(mapping_path, 'rb') as f:
                    mappings = pickle.load(f)
                    self.kg_e2id = mappings['entity2id']
                    self.kg_r2id = mappings['relation2id']
                    self.kg_id2e = mappings['id2entity']
                
                print(f"[Inference] Loading KG model from {model_path}")
                # Use same dims as training script
                self.kg_model = TuckER(
                    len(self.kg_e2id),
                    len(self.kg_r2id),
                    d1=200, d2=200, # Default per train_tucker_kg.py
                    input_dropout=0.3, hidden_dropout1=0.4, hidden_dropout2=0.5
                ).to('cpu') # Inference on CPU is fine
                
                self.kg_model.load_state_dict(torch.load(model_path, map_location='cpu'))
                self.kg_model.eval()
                self.kg_loaded = True
                print("[Inference] KG Model loaded successfully!")
            else:
                print("[Inference] KG model/mapping not found. Run reproduce/train_tucker_kg.py first.")
        except Exception as e:
            print(f"[Inference] Error loading KG model: {e}")
            import traceback
            traceback.print_exc()

    def predict_kg_tail(self, head_entity: str, relation: str) -> list:
        """çŸ¥è­˜ã‚°ãƒ©ãƒ•ã§æ¨è«– (Head, Relation, ?) -> Top 3 Tails"""
        if not self.kg_loaded:
            return []
            
        # å®‰å…¨ç­–: éƒ¨åˆ†ä¸€è‡´ã§ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’æ¢ã™
        h_id = self.kg_e2id.get(head_entity)
        if h_id is None:
            # ç°¡æ˜“ã‚µãƒ¼ãƒ
            for name, eid in self.kg_e2id.items():
                if head_entity in name or name in head_entity:
                    h_id = eid
                    break
        
        r_id = self.kg_r2id.get(relation)
        
        if h_id is not None and r_id is not None:
            try:
                with torch.no_grad():
                    h_tensor = torch.tensor([h_id])
                    r_tensor = torch.tensor([r_id])
                    
                    pred = self.kg_model.forward(h_tensor, r_tensor)
                    # Get top 3
                    scores, indices = torch.topk(pred, 3)
                    
                    results = []
                    for idx in indices[0]:
                        ent_name = self.kg_id2e.get(idx.item(), "Unknown")
                        results.append(ent_name)
                    return results
            except Exception as e:
                print(f"[Inference] KG Predict Error: {e}")
        
        return []

    def set_category(self, category_idx: int) -> dict:

        """ã‚«ãƒ†ã‚´ãƒªã‚’è¨­å®š"""
        if 0 <= category_idx < len(CATEGORY_LIST):
            self.current_category = category_idx
            self.current_product = None
            self.dialogue_history = []
            
            products = CATEGORY_PRODUCTS.get(category_idx, [])
            return {
                "success": True,
                "category_name": CATEGORY_LIST[category_idx],
                "category_id": category_idx,
                "products": products,
                "message": f"ã€Œ{CATEGORY_LIST[category_idx]}ã€ã‚«ãƒ†ã‚´ãƒªãŒé¸æŠã•ã‚Œã¾ã—ãŸã€‚å•†å“ã‚’ãŠé¸ã³ãã ã•ã„ã€‚"
            }
        return {"success": False, "error": "ç„¡åŠ¹ãªã‚«ãƒ†ã‚´ãƒªã§ã™"}
    
    def set_product(self, product_name: str) -> dict:
        """å•†å“ã‚’è¨­å®šï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®è©³ç´°æƒ…å ±ã‚’ä½¿ç”¨ï¼‰"""
        if self.current_category is None:
            return {"success": False, "error": "å…ˆã«ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠã—ã¦ãã ã•ã„"}
        
        category_name = CATEGORY_LIST[self.current_category]
        products = CATEGORY_PRODUCTS.get(self.current_category, [])
        
        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«ã‚ã‚‹å•†å“ã‚‚å«ã‚ã‚‹
        real_products = list(REAL_PRODUCT_DATA.get(category_name, {}).keys())
        all_products = list(set(products + real_products))
        
        if product_name in all_products:
            self.current_product = product_name
            self.dialogue_history = []
            
            # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æƒ…å ±ã‚’å–å¾—
            product_info = REAL_PRODUCT_DATA.get(category_name, {}).get(product_name, {})
            self.current_params = product_info.get("params", {})
            self.current_qa_list = product_info.get("qa", [])
            
            # ä¾¡æ ¼è¨­å®š
            price_str = self.current_params.get("price", "9,800å††")
            # "9,800å††" -> 9800
            try:
                self.current_price = int(price_str.replace(",", "").replace("å††", ""))
            except:
                self.current_price = 9800
            
            return {
                "success": True,
                "product": product_name,
                "price": self.current_price,
                "category": category_name,
                "message": f"ã€Œ{product_name}ã€ãŒé¸æŠã•ã‚Œã¾ã—ãŸã€‚ã”è³ªå•ã‚’ã©ã†ãã€‚"
            }
        return {"success": False, "error": f"ã€Œ{product_name}ã€ã¯é¸æŠã§ãã¾ã›ã‚“"}

    def _find_best_match_qa(self, query: str) -> str:
        """è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æœ€ã‚‚é¡ä¼¼ã—ãŸè³ªå•ã¸ã®å›ç­”ã‚’æ¤œç´¢"""
        if not self.current_qa_list:
            return None
            
        best_ratio = 0.0
        best_answer = None
        
        from difflib import SequenceMatcher
        
        for qa in self.current_qa_list:
            q_text = qa["q"]
            # å˜ç´”ãªé¡ä¼¼åº¦è¨ˆç®—
            ratio = SequenceMatcher(None, query, q_text).ratio()
            if ratio > best_ratio:
                best_ratio = ratio
                best_answer = qa["a"]
        
        # é–¾å€¤ã‚’è¨­å®šï¼ˆã‚ã¾ã‚Šã«ä½ã„å ´åˆã¯ãƒãƒƒãƒã—ãªã„ã¨ã™ã‚‹ï¼‰
        if best_ratio > 0.6:
            return best_answer
        return None

    def predict_satisfaction(self, text: str) -> int:
        """ãƒ«ãƒ¼ãƒ« + ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦æº€è¶³åº¦ã‚’äºˆæ¸¬"""
        
        # ã¾ãšã€æ˜ç¢ºãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã§ãƒã‚§ãƒƒã‚¯ï¼ˆæœ€å„ªå…ˆï¼‰
        rule_result = self._check_obvious_sentiment(text)
        if rule_result is not None:
            return rule_result
        
        # ãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã¯ã€ãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬
        if self.model is not None and self.tokenizer is not None:
            try:
                full_text = " ||| ".join(self.dialogue_history[-5:] + [text])
                
                encoded = self.tokenizer(
                    full_text,
                    max_length=256,
                    truncation=True,
                    padding='max_length',
                    return_tensors='pt'
                )
                
                input_ids = encoded['input_ids'].to(self.device)
                attention_mask = encoded['attention_mask'].to(self.device)
                topic = torch.tensor([self.current_category or 0]).to(self.device)
                
                with torch.no_grad():
                    logits = self.model(input_ids, attention_mask, topic)
                    probs = torch.softmax(logits, dim=1)
                    prediction = torch.argmax(logits, dim=1).item()
                    confidence = probs[0, prediction].item()
                
                # --- Safety Net for Model Prediction ---
                # ãƒ¢ãƒ‡ãƒ«ãŒã€Œä¸æº€(0)ã€ã¨äºˆæ¸¬ã—ã¦ã‚‚ã€å®¢è¦³çš„ãªè³ªå•ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚„ã‚¯ãƒƒã‚·ãƒ§ãƒ³è¨€è‘‰ãŒå«ã¾ã‚Œã€ã‹ã¤å¼·ã„ãƒã‚¬ãƒ†ã‚£ãƒ–èªãŒãªã„å ´åˆã¯ã€Œæ™®é€š(1)ã€ã«è£œæ­£
                if prediction == 0:
                    is_objective = any(k in text for k in OBJECTIVE_KEYWORDS)
                    is_polite = any(k in text for k in POLITE_IGNORE_KEYWORDS)
                    has_strong_negative = any(k in text for k in NEGATIVE_KEYWORDS)
                    
                    if (is_objective or is_polite) and not has_strong_negative:
                        print(f"[Inference] Override model prediction 0 -> 1 (Objective/Polite: {text})")
                        return 1

                # ä¿¡é ¼åº¦ãŒä½ã„å ´åˆã¯ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                if confidence < 0.5:
                    return self._predict_satisfaction_rule_based(text)
                
                return prediction
                
            except Exception as e:
                print(f"[Inference] Model prediction error: {e}")
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹
        return self._predict_satisfaction_rule_based(text)
    
    def _check_obvious_sentiment(self, text: str) -> int:
        """æ˜ç¢ºãªæ„Ÿæƒ…è¡¨ç¾ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆæœ€å„ªå…ˆï¼‰"""
        
        # å¼·ã„ãƒã‚¬ãƒ†ã‚£ãƒ–è¡¨ç¾ (èª¤æ¤œçŸ¥ã‚’é˜²ããŸã‚ã€æ˜ã‚‰ã‹ã«æ„Ÿæƒ…çš„ãªã‚‚ã®ã«é™å®š)
        strong_negative = [
            "æœ€æ‚ª", "ã²ã©ã„", 
            "ã‚¯ãƒ¬ãƒ¼ãƒ ", "æ€’", "è…¹ç«‹", "å¤±æœ›", "æ®‹å¿µ", "ãµã–ã‘", "äºŒåº¦ã¨", "é‡‘è¿”ã›", "è©æ¬º",
            "å¯¾å¿œãŒæ‚ªã„", "æ…‹åº¦ãŒæ‚ªã„"
        ]
        # æ–‡è„ˆä¾å­˜ã®å˜èª ("è¿”å“"ãªã©) ã¯ãƒªã‚¹ãƒˆã‹ã‚‰é™¤å¤–
        
        for keyword in strong_negative:
            if keyword in text:
                return 0  # ä¸æº€
        
        # å®¢è¦³çš„ãªè³ªå•ã®å ´åˆã€ã“ã“ã§ã€Œæ™®é€šã€ã¨ç¢ºå®šã•ã›ã‚‹ (Safety Net 1)
        # ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«ã‚„ä»–ã®ãƒ«ãƒ¼ãƒ«ãŒèª¤ã£ã¦ä¸æº€ã¨åˆ¤å®šã™ã‚‹ã®ã‚’é˜²ã
        is_objective = any(k in text for k in OBJECTIVE_KEYWORDS)
        is_polite = any(k in text for k in POLITE_IGNORE_KEYWORDS)
        has_negative_context = any(k in text for k in NEGATIVE_KEYWORDS)
        
        if (is_objective or is_polite) and not has_negative_context:
             return 1 # æ™®é€š
        
        # å¼·ã„ãƒã‚¸ãƒ†ã‚£ãƒ–è¡¨ç¾
        strong_positive = [
            "ã‚ã‚ŠãŒã¨ã†", "æ„Ÿè¬", "åŠ©ã‹ã‚Š", "å¬‰ã—ã„", "æº€è¶³", "æœ€é«˜",
            "ç´ æ™´ã‚‰ã—ã„", "å®Œç’§", "ãŠã™ã™ã‚", "æ°—ã«å…¥", "è‰¯ã‹ã£ãŸ", "ã„ã„æ„Ÿã˜"
        ]
        for keyword in strong_positive:
            if keyword in text:
                return 2  # æº€è¶³
        
        return None  # æ˜ç¢ºãªåˆ¤æ–­ãŒã§ããªã„å ´åˆ
    
    def _predict_satisfaction_rule_based(self, text: str) -> int:
        """ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®æº€è¶³åº¦äºˆæ¸¬ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
        # ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒã‚§ãƒƒã‚¯
        for keyword in NEGATIVE_KEYWORDS:
            if keyword in text:
                return 0
        
        # ãƒã‚¸ãƒ†ã‚£ãƒ–ãƒã‚§ãƒƒã‚¯
        for keyword in POSITIVE_KEYWORDS:
            if keyword in text:
                return 2
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯æ™®é€š
        return 1
    
    def detect_intent(self, text: str) -> str:
        """ã‚¤ãƒ³ãƒ†ãƒ³ãƒˆã‚’æ¤œå‡º"""
        text_lower = text.lower()
        
        # æŒ¨æ‹¶
        if any(w in text for w in ["ã“ã‚“ã«ã¡ã¯", "ãŠã¯ã‚ˆã†", "ã“ã‚“ã°ã‚“ã¯", "ã¯ã˜ã‚ã¾ã—ã¦", "ã‚ˆã‚ã—ã"]):
            return "greeting"
        
        # æ„Ÿè¬
        if any(w in text for w in ["ã‚ã‚ŠãŒã¨ã†", "ã‚µãƒ³ã‚­ãƒ¥ãƒ¼", "æ„Ÿè¬", "åŠ©ã‹ã‚Š"]):
            return "thanks"
        
        # ã‚¯ãƒ¬ãƒ¼ãƒ 
        if any(w in text for w in NEGATIVE_KEYWORDS[:10]):
            return "complaint"
        
        # ä¾¡æ ¼
        if any(w in text for w in ["ã„ãã‚‰", "å€¤æ®µ", "ä¾¡æ ¼", "å††", "ãŠé‡‘", "ã‚³ã‚¹ãƒˆ", "ãªã‚“ã¼", "ãŠã„ãã‚‰"]):
            return "price"
        
        # åœ¨åº«
        if any(w in text for w in ["åœ¨åº«", "ã‚ã‚‹", "ã‚ã‚Šã¾ã™ã‹", "å…¥è·", "å“åˆ‡ã‚Œ", "å£²ã‚Šåˆ‡ã‚Œ", "ã”ã–ã„ã¾ã™ã‹"]):
            return "stock"
        
        # é…é€
        if any(w in text for w in ["å±Šã", "å±Šã", "é…é€", "ç´æœŸ", "ç™ºé€", "ã„ã¤å±Šã", "é…é”", "ä½•æ—¥"]):
            return "delivery"
        
        # ã‚¹ãƒšãƒƒã‚¯
        if any(w in text for w in ["ã‚µã‚¤ã‚º", "å¯¸æ³•", "é‡ã•", "é‡é‡", "ã‚¹ãƒšãƒƒã‚¯", "ä»•æ§˜", "å¤§ãã•"]):
            return "spec"
        
        # ãŠã™ã™ã‚
        if any(w in text for w in ["ãŠã™ã™ã‚", "ã‚ªã‚¹ã‚¹ãƒ¡", "é¸ã³æ–¹", "ã©ã‚ŒãŒã„ã„", "æ¯”è¼ƒ", "äººæ°—"]):
            return "recommend"
        
        # è¿”å“
        if any(w in text for w in ["è¿”å“", "äº¤æ›", "ã‚­ãƒ£ãƒ³ã‚»ãƒ«", "è¿”é‡‘"]):
            return "return"
        
        # å“è³ª
        if any(w in text for w in ["å“è³ª", "ä¸ˆå¤«", "é•·æŒã¡", "è€ä¹…", "ä¿è¨¼"]):
            return "quality"
        
        return "fallback"

    def generate_response(self, message: str) -> dict:
        """å¿œç­”ã‚’ç”Ÿæˆï¼ˆRAG + ãƒ«ãƒ¼ãƒ« + ãƒ¢ãƒ‡ãƒ«ï¼‰"""
        # å¯¾è©±å±¥æ­´ã«è¿½åŠ 
        self.dialogue_history.append(f"Q: {message}")
        
        # æ„å›³æ¤œå‡º (å…±é€šã§ä½¿ç”¨)
        detected_intent = self.detect_intent(message)
        
        # 1. è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®æ¤œç´¢ (Retrieval)
        retrieved_answer = self._find_best_match_qa(message)
        
        # 2. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¤œç´¢ (Spec retrieval)
        extracted_param_ans = None
        if not retrieved_answer:
            # ç°¡æ˜“çš„ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æŠ½å‡º
            if "ä¾¡æ ¼" in message or "ã„ãã‚‰" in message:
                if "price" in self.current_params:
                    extracted_param_ans = f"ä¾¡æ ¼ã¯{self.current_params['price']}ã§ã™ã€‚"
            elif "é‡ã•" in message or "é‡é‡" in message:
                 if "weight" in self.current_params:
                    extracted_param_ans = f"é‡é‡ã¯{self.current_params['weight']}ã§ã™ã€‚"
            elif "ã‚µã‚¤ã‚º" in message or "å¯¸æ³•" in message or "å¤§ãã•" in message:
                 if "size" in self.current_params:
                    extracted_param_ans = f"ã‚µã‚¤ã‚ºã¯{self.current_params['size']}ã§ã™ã€‚"
            
            if extracted_param_ans:
                retrieved_answer = extracted_param_ans

        # æº€è¶³åº¦äºˆæ¸¬ (å…±é€šå‡¦ç†)
        if self.model_loaded:
            satisfaction = self.predict_satisfaction(message)
        else:
            satisfaction = self._predict_satisfaction_rule_based(message)

        # 3. çŸ¥è­˜ã‚°ãƒ©ãƒ•æ¨è«– (Reasoning)
        kg_insight = None
        if self.kg_loaded and self.current_product:
            # ã‚¤ãƒ³ãƒ†ãƒ³ãƒˆã‹ã‚‰é–¢ä¿‚æ€§ã‚’ãƒãƒƒãƒ”ãƒ³ã‚°
            rel_map = {
                "price": "å±æ€§", # "ä¾¡æ ¼"é–¢ä¿‚ãŒãªã„ã®ã§å±æ€§ã¨ã—ã¦æ¨è«–
                "spec": "å±æ€§",
                "quality": "å±æ€§",
                "recommend": "ã‚«ãƒ†ã‚´ãƒª"
            }
            target_rel = rel_map.get(detected_intent, "å±æ€§")
            
            kg_preds = self.predict_kg_tail(self.current_product, target_rel)
            if kg_preds:
                kg_insight = f"ã€AIæ¨è«–ã€‘çŸ¥è­˜ã‚°ãƒ©ãƒ•ã«ã‚ˆã‚‹ã¨ã€{self.current_product}ã¯ã€Œ{', '.join(kg_preds)}ã€ã¨é–¢é€£ãŒã‚ã‚Šã¾ã™ã€‚"

        # å¿œç­”ã®æ±ºå®š
        if retrieved_answer:
            response = retrieved_answer
            intent = "retrieval"
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ—¢å­˜ã®ãƒ­ã‚¸ãƒƒã‚¯
            intent = detected_intent
            
            # å¿œç­”ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’é¸æŠ
            templates = RESPONSE_TEMPLATES.get(intent, RESPONSE_TEMPLATES["fallback"])
            response_template = random.choice(templates)
            
            # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå¤‰æ•°ã‚’ç½®æ›
            product = self.current_product or "å•†å“"
            category = CATEGORY_LIST[self.current_category] if self.current_category is not None else "å•†å“"
            price = self.current_price or 9802

            
            response = response_template.format(
                product=product,
                category=category,
                price=f"{price:,}"
            )

        
        # KGæ¨è«–çµæœã‚’ä»˜ä¸
        if kg_insight:
            response += f"\n\n{kg_insight}"
        
        sat_info = SATISFACTION_LABELS[satisfaction]

        
        # å¯¾è©±å±¥æ­´ã«è¿½åŠ 
        self.dialogue_history.append(f"A: {response}")
        
        return {
            "response": response,
            "satisfaction": satisfaction,
            "satisfaction_label": f"{sat_info['label']} {sat_info['emoji']}",
            "satisfaction_class": sat_info['class'],
            "category": CATEGORY_LIST[self.current_category] if self.current_category is not None else "Unknown",
            "product": self.current_product or "Unknown",
            "intent": intent,
            "model_based": self.model_loaded,
        }
    
    def get_categories(self) -> list:
        """ã‚«ãƒ†ã‚´ãƒªä¸€è¦§ã‚’å–å¾—"""
        return [{"id": i, "name": cat} for i, cat in enumerate(CATEGORY_LIST)]
    
    def get_products(self, category_id: int) -> list:
        """æŒ‡å®šã‚«ãƒ†ã‚´ãƒªã®å•†å“ä¸€è¦§ã‚’å–å¾—"""
        return CATEGORY_PRODUCTS.get(category_id, [])
    
    def reset_dialogue(self):
        """å¯¾è©±ã‚’ãƒªã‚»ãƒƒãƒˆ"""
        self.dialogue_history = []
        return {"success": True, "message": "å¯¾è©±å±¥æ­´ãŒãƒªã‚»ãƒƒãƒˆã•ã‚Œã¾ã—ãŸ"}


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
_inference_engine = None


def get_inference_engine():
    """ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—"""
    global _inference_engine
    if _inference_engine is None:
        _inference_engine = MonotaROInference()
    return _inference_engine




def generate_response(message: str) -> dict:
    """ä¾¿åˆ©é–¢æ•°"""
    engine = get_inference_engine()
    return engine.generate_response(message)


# ãƒ†ã‚¹ãƒˆ
if __name__ == "__main__":
    engine = MonotaROInference()
    
    print("\n" + "=" * 60)
    print(" MonotaRO Q&A Inference Test (Model-Based)")
    print("=" * 60)
    
    # ã‚«ãƒ†ã‚´ãƒªé¸æŠ (ãƒ˜ãƒ«ãƒ¡ãƒƒãƒˆã‚’æ¢ã™)
    target_product = "ãƒ˜ãƒ«ãƒ¡ãƒƒãƒˆ"
    target_cat_id = -1
    
    for i, cat in enumerate(CATEGORY_LIST):
        products = engine.get_products(i)
        if target_product in products:
            target_cat_id = i
            break
    
    if target_cat_id != -1:
        result = engine.set_category(target_cat_id)
        print(f"\n>>> {result.get('message', 'ã‚«ãƒ†ã‚´ãƒªé¸æŠå¤±æ•—')}")
        
        # å•†å“é¸æŠ
        result = engine.set_product(target_product)
        print(f"\n>>> {result.get('message', result.get('error'))}")
    else:
        print(f"Product {target_product} not found in any category.")
        # Fallback to category 0
        engine.set_category(0)
        engine.set_product(engine.get_products(0)[0])

    
    # ãƒ†ã‚¹ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    test_messages = [
        "ã“ã®ãƒ˜ãƒ«ãƒ¡ãƒƒãƒˆã¯ã„ãã‚‰ã§ã™ã‹ï¼Ÿ",
        "åœ¨åº«ã¯ã”ã–ã„ã¾ã™ã‹ï¼Ÿ",
        "ã„ã¤å±Šãã¾ã™ã‹ï¼Ÿ",
        "è€è·é‡ã¯ã©ã‚Œãã‚‰ã„ã§ã™ã‹ï¼Ÿ",
        "ã™ã¿ã¾ã›ã‚“ã€æ£šã«ã¤ã„ã¦è³ªå•ãªã‚“ã§ã™ãŒã€‚", # User reported: Polite start -> should be Neutral
        "æœ€å¤§å›è»¢æ•°ã¯ï¼Ÿ", # User reported: Technical spec -> sould be Neutral
        "ãŠå¿™ã—ã„ã¨ã“ã‚ã™ã¿ã¾ã›ã‚“", # User reported: Polite apology -> should be Neutral
        "ãƒã‚¤ãƒ³ãƒˆä½•å€ï¼Ÿ", # User reported: Points inquiry -> should be Neutral
        "å•†å“ãŒå£Šã‚Œã¦ã„ã¾ã—ãŸã€‚è¿”å“ã—ãŸã„ã§ã™ã€‚",
        "ã¨ã¦ã‚‚åŠ©ã‹ã‚Šã¾ã—ãŸã€‚ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ï¼",
    ]
    
    for msg in test_messages:
        result = engine.generate_response(msg)
        print(f"\n[Q] {msg}")
        print(f"[A] {result['response']}")
        print(f"    æº€è¶³åº¦: {result['satisfaction_label']}")
        print(f"    ãƒ¢ãƒ‡ãƒ«æ¨è«–: {result['model_based']}")
